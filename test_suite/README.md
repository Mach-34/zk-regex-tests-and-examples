# Test suite for zk-regex + Noir

This test-suite is a tool for testing and benchmarking the zk-regex library module that generates Noir code to check if certain regex is matched.

In addition, all the tests for which the circom implementation of zk-regex gets tested are implemented. A large part can be ran automatically through the automated testing/benchmarking functionality for which a database has to be provided (and `regex_db_for_bench.json` contains all the necessary testdata) and there are 2 testcases that have been implemented separately in `hardcoded_tests`.  The latter is necessary because for those tests multiple circuits have to be combined manually. 

## Requirements

- Install zk-regex command following the instructions in the documentation.
- Install Noir.
- Install the `hyperfine` tool.

## How to run

To execute the testing and/or the benchmarking, you need to fill a JSON database that will be used for the testing and/or benchmarking. The testing and the benchmarking are two separate processes. This means that you may execute the testing without the benchmarking, or the benchmarking without the testing, or execute both the testing and the benchmarking. However, both processes will use the same database for their ends. It is important to add that if you want to execute just the testing or just the benchmarking, some of the fields are not mandatory as we will explain next. The basic structure for the database is as follows:

```json
{
  "database": [
    <sample>,
    <sample>,
    <sample>,
  ]
}

```

where `<sample>` is a JSON object with certain required fields. The instructions to fill the database are presented below.

## Instructions for testing

Each sample for testing can be one of two types: raw, or decomposed. "Raw" means that the regex is specified as just one string, while "decomposed" means that the regex is specified in fragments according to the zk-regex specification. Below you will find an example of how to specify a database with both types of specification. It is important to notice that the database can have samples with a mixture of both types: some samples may be "raw", and other samples may be "decomposed".

```json
{
    "database": [
        {
            "regex": {
                "raw": "m(a|b)+-(c|d)+e$"
            },
            "input_size": 16,
            "samples_pass": [
                "mabab-cdcde",
                "ma-ce"
            ],
            "samples_fail": [
                "sdjfalsdjflasjf",
                "slafjsajdflasjd"
            ]
        },
        {
            "regex": {
                "decomposed": [
                    {
                        "is_public": false,
                        "regex_def": "ab"
                    },
                    {
                        "is_public": true,
                        "regex_def": "cd"
                    }
                ]
            },
            "input_size": 16,
            "samples_pass": [
                "abcd"
            ],
            "samples_fail": [
                "abw",
                "cdf"
            ]
        }
    ]
}
```

To execute the tests, you simply run.

```bash
RUST_LOG=info cargo run -- -t
```

## Instructions for benchmarking

This tool allows you to benchmark the source code to evaluate the performance. The benchmarking requires to add additional flags and information to the database presented in the previous section. It is important to make clear that if you just want to execute the test, the modifications to the database associated with the benchmark **are not mandatory**.

The benchmarking tool supports the following features:

- If you want to benchmark all the samples, you can add the `"bench_all": true` to the database as follows:

  ```json
  {
    "bench_all": true,
    "database": [
      <sample>,
      <sample>,
      <sample>,
    ]
  }
  ```

  This flag is optional.
- You may want to execute benchmarks not for all the samples but for some of them. If you want to execute the benchmark for some of the samples but not for all, you can add the flag `"with_bench": true` to benchmark that sample. This flag is considered optional, and if you do not add it, the default value will be considered as `false`.
- There are two types of benchmarking:
  - *without time*: this mode of benchmarking means that the report will include just the gate-count of the circuit generated by the zk-regex library, but this mode does not measure the proving time. To execute the benchmarking in this mode you must execute the following command:

    ```bash
    RUST_LOG=info cargo run -- [OPTIONS] no-time
    ```

  - *with time*: this mode of benchmarking measures the gate-count as in the previous mode, and also measures the proving time of 5 executions of the Noir project. To execute this you need to add the field `"benchmark_str"` with a string that must successfully match the regex. This string will be used as the witness for the prove. If you **do not** want to benchmark the proving time, you can omit this field in the database. To run the benchmarking *with time*, you must execute the following command:

    ```bash
    RUST_LOG=info cargo run -- [OPTIONS] with-time
    ```

If you want to execute the benchmarking **without** the testing, you can omit the `-t` flag in the execution command as shown below:

```bash
RUST_LOG=info cargo run -- <no-time | with-time>
```

## Execution of testing and benchmarking simultaneously

If you want to execute both the testing and the benchmarking you need to follow the instructions presented above for the benchmarking and the testing independently. Then you can execute the following command:

```bash
RUST_LOG=info cargo run -- -t <no-time | with-time>
```

## Circom testing compatibility

The file `regex_db_for_bench.json` contains all testcases that the [circom implementation tests](https://github.com/zkemail/zk-regex/tree/main/packages/circom/tests) for and some additional ones. 

The circom tests are contained in `.test.js` files and usually test for various circuits within a single file. In `regex_db_for_bench.json` the information is organized per circuit, but we've added information to link them back to the circom tests. If there's a specific circuit file it relates to, this is indicated with `circuit_name`, otherwise there is a reference in `test_name`. Examples: 
```json
"circuit_name": "asterisk1"

"test_name": "simple_regex.test.js"
``` 

Furthermore, to relate the test inputs to the specific testcases, `circom_testname` is indicated for passing samples, and separately in an array for failing samples. E.g:

```json
      "samples_pass": [
        { 
          "input": "xb", 
          "expected_substrings": [],
          "circom_testname": "asterisk1 valid case 1"
        },
        { 
          "input": "xab", 
          "expected_substrings": ["a"],
          "circom_testname": "asterisk1 valid case 2"
        }
      ],
      "samples_fail": [
        "xaaa",
        "aaabx"
      ],
      "circom_testnames_fails": [
        "asterisk1 invalid case 1",
        "asterisk1 invalid case 2"
      ]
```

These labels make it easier to verify whether all circom tests have been implemented. 

Note that as mentioned in the introduction, there are a few tests that are implemented in `hardcoded_tests` as they combine multiple circuits, which cannot be done through the automated process. 

## Limitations

For some regexes the random sampling is not possible, because the sampling library is limited. For example the end anchor (`$`) is not supported.

Random sample testing for the `gen_substrs` setting is only support for `decomposed`. In the `raw` setting, the substrings are determined via a json file that contains the transition information. Determining what the substring parts are, would be quite involved since it requires building the DFA.

